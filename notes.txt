Not sure where else to put this, since it's not that useful. My SRC_ID is v81v7558

DESIRED PROGRAM FLOW:
1) SRC endpoints like levels/categories/runs that I require get their own dedicated
   scrape functions that handle getting data onto your machine
2) These functions call my `load_data` function, which handles interfacing with the API
   and applying enrichment functions
   a) Pre-enriched data can also be read locally here as a sort of cache, but this is
      probably not necessary. generate_graphs already reads parquet directly so this
      option just muddies the call-chain
   b) It calls the API by building an arg string with build_arg_str and then the big one,
      query_api
   c) Enrich the data with the enrich function provided to load_data
   d) Save the data to disk
   e) return the enriched DF if you want to use it for more
3) `query_api` is the big one, this handles actually talking with the src API
   a) I think the big thing I need to do here is define a specific output for this function.
      the weirdness mostly comes with parsing out paginated results into one, and what
      parts of the raw HTTP I strip out vs what gets returned

JULY 2023 GITHUB GLOW UP:
- The goal is to get this into a state where I can pass this along to others 
  and allow them to add new graphs and features and just play around with it
* The biggest thing I want to do is separate enrichment into its own python script
  and make all enriched columns more obvious, probably by prepending `e_` to them
  - Push the `format_results` functionality into the scraper so that your api query
    function handles surfacing a consistent view of the data without any of the HTTP
    wrappers
  - I may need to figure out accessing list elements in pandas to handle the new 1P/2P
    stuff, since right now I just extract 1P and idk if just extracting 2P as well is
    sufficient.
  - I don't remember why I did the dummy thing but I'm pretty sure it was important lol
- Try to store data in its original format and with no deletions (players list comes 
  to mind)
- Find globals and put them in a script for easy tuning
- Add Pep/Noise/Swap column that parses run type
  - You may want this to be more generic later on but for now it's just for PT
- Add ruff as a pre-commit hook
- Write a tutorial for getting this running in WSL as part of the readme
+ Add sleeps to the user-fetch function
- Stretch: rewrite the Query API to be a little more generic
  * Quickly refamiliarize yourself with the v1/v2 src endpoints and their shortcomings
    ! Oh hey, you can increace pagination, look into that
  - Start explicitly handling pagination in query_api so that it's invisible outside of it
  + Read the Real Python tutorial on the requests library. It's a diversion but will
    hopefully help you clean up query_api a lot
    + This helped me understand passing args, neat!
- Stretch: See what it would take to get this running on Windows

FUTURE WORK:
- Fix the runs per week graph
  - Make the month bars be in the back
  - Make the month bars automagically
- New Players per month graph?
  - Just get the first run submitted by all players and count up
- Maybe a way for you to see all new WRs in the month, which you can turn into
  a news post
- A List of all countries that have submitted runs, this may require pulling more
  from the user-cache and a biiiig long pull that I should probably amortize
- New runs per month broken down by category
- John Gutter Special:
  - Second Barriers
  - WR history


TODOs for January Stats:
+ Category split! Update your models to take in subcategories
  + Grab and save variables from the games/{ID}/variables endpoint, probably pickled?
    it's a multilevel dict
  + Link in that variable dict to get the glitched status of the run
- Gotta figure out how I want to represent this, probably in front of 
- Truncate the runs per week graph (basically done, just noting it)
- I should make the New Runners graph I've been thinking about



TODOs for December stats:
- Graph list:
  - Runs per week
  - Minute barriers Any%/100%
    - Mark new broken barriers
  - Long-standing records/active records
- I want to mark new runs that break minute barriers
  - To do this I need to check if the runner has broken this minute barrier
    since the last stats post, which I guess I can feed as an argument
- I should un-cache the run join since I may want to filter/not filter different
  datasets for different graphs
- I also wanna get the runs per week bars automatic
  - Also maybe behind the bars but that's probably not a one-day thing
- Maybe in the future a function to count the number of new run submitters
  in the month

TOMORROW: Build that shit live for the post, but then change the functionality
  so the function uses the subplot axes functionality. Should help the weekly
  graph as well

Overall TODO List (Novemeber 1000 member event):
- Refactoring: 
  + Add DATA_PATH, CHART_PATH, and the init function that creates them to the utils.py script
  + Save figures in the functions that generate graphs
  + Remove all them prints from the run DL script, you know it's working now
  + Move the short_names list to the get_levels script
  + Remove Stupid Rat runs
  + Remove Rejected Runs
  + Parameterize the download scripts so they can pull from the CE board
  + Maybe rework the entire query_api path? idk, the API is a bit weird regardless
  - Update the leaderboard download/enrichment
    - this will likely require updating how we write files
+ Make a CSV export function for the joined data
+ Create (recreate?) the top 5 submitters code
+ Get the CE API/category names/etc, maybe move all the labeling stuff into its own config file (or utils, lol)
+ Decide how to save files from different games (CE/main)
  + Probably keep all PT in the same file
  + Maybe make the dl functions take a list of identifiers and then concat them together?
  + Hmm, actually, I think I'd rather join them at graphing time, esp given the size disparities
    and the fact that to join PT/PT CE categories I'd need to download the whole main board twice, lol
- Parameterize and append different datasets together in join_all_data
  - I'll probably leave this off until after this event and just do main boards
+ Create a col that marks the "era" of a category (ie. original, Halloween, category extensions)
  + this likely belongs in the category/level files
+ Make a Top 5 ILs graph for each category
+ Create a function that returns only WRs
+ Make a Longest-Lasting Records Graph
  - I've got the list now, but I should think how I want to display it
  - Maybe another horizontal Bar Graph with active records in a different color
- Add impossa's background image
  - Make all images transparent and put the background image behind them
- Rework the colors to be a bit more pizza-towery
- Add Peppino screaming to the all levels graph, for JG
- Get the background bars to draw before the graph
  - Currently the function creates a new subplot, if we just reuse the same one in our functions this should work
  - This might also help me auto-do background images
- Programmatically create the background bars that differentiate months in the il runs

XBC's TODOs:
+ Add bars to the months in the IL runs graph
- Remove the "Full Game" tag from the run titles
- Filter the full run list down to get leaderboards
- Make the Minute Barriers graph off those filtered lists

New Graph Ideas:
- Cumulative Runs By Day graph
- Cumulative Players By Day graph

2023-11-02:
- Made a CSV export for XBC, and now I can export whatever I want with the export_joined_runs_csv 
  function
- Spent mayyyybe a bit too much time on the username cache but hopefully now it'll mean I just
  get names for free

2023-10-31:
- If i was to split it out even more would it be one script for scraping and another 
  for cleaning the data? is that worth doing?
  - I think for now, meh, but worth keeping in mind
- It may make sense to make a local vs remote read function for loading in these datasets
  so I'm not pulling the entire run history every time i want to load runs into jupyter
- Then of course we have the question of generic loads/api read functions to avoid repeating meself
- So the thing about generifying the data load functions is that the ordering is a bit weird:
  - If local is true, just load the existing file, no changes
  - If not, query the API, format the results
  - perform custom enhancement per dataset
  - I want to generify both the local load and the non-local pull but enhancement only happens
    when non-local is called
  - I guess I could pass local to both bits?
  - Of course local vs api reads have totally different parameters, but i could store those
    in a structure
- I think I got it: a generic function for local/api/formatting calls, and functions
  for cleaning up each dataset and turning it into a dataframe
- Whew, got it!

2023-10-29 Suggestion Graphs:
- Top ILs per category (top 5?)
- Cumulative Total Runs

2023-10-28 TODO List:
- Figure out how to save the graphs I make
- Start pulling from the category extensions board
- Mark SotW and Pumpkin runs as part of the halloween update
- Mark category extensions as, well, category extensions
- Build and post the per-week, per-level, and minute-barrier graphs to the mod channel
- Investigate putting a break in the per-level board so JG doesn't take over everything


So what stats do I actually want to surface with my data? (IL vs Full game separately)
- Runs per day (graphed)
  - Need to group by day, count IL = true vs false
  - Maybe runs per week to look less crowded?
- Runners per month
- New runners per month
- Run minute histogram
- Total time submitted per month
- Runners who have submitted the most runs?
- Day of the week that gets the most submissions
- Record progression by category? Available on src but might be nice
- Average run time per week per category?

- Most run ILs
- Least run ILs
  - just one full ranking, right?

- Verifier stats? Maybe just for me, lol

TODO list:
- Clean out stupid rat (done!)
- Remove pending/rejected runs (Marked!)
- Make a per-week runs chart
- Make a runners per month chart

